import boto3
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

BUCKET_NAME = 'your-bucket'
FILE_KEY = 'your-key'

# Task 1: Write data to S3
def write_to_s3(**kwargs):
    s3 = boto3.client('s3')
    data = b"This is large data"
    s3.put_object(Bucket=BUCKET_NAME, Key=FILE_KEY, Body=data)
    print(f"Data written to S3: {FILE_KEY}")

# Task 2: Read data from S3
def read_from_s3(**kwargs):
    s3 = boto3.client('s3')
    response = s3.get_object(Bucket=BUCKET_NAME, Key=FILE_KEY)
    data = response['Body'].read()
    print(f"Data read from S3: {data.decode()}")

# DAG definition
with DAG(
    dag_id='s3_example',
    start_date=datetime(2023, 1, 1),
    schedule_interval=None,
) as dag:
    write_task = PythonOperator(
        task_id='write_to_s3',
        python_callable=write_to_s3,
    )
    read_task = PythonOperator(
        task_id='read_from_s3',
        python_callable=read_from_s3,
    )

    write_task >> read_task
